name: llama_finetuning
edges:
  - source: create_dataset
    target: train
    data:
      source_path: /mnt/data
      target_path: sky_llama/dataset
      size_gb: 10

---
name: create_dataset

resources:
  cpus: 2+
  memory: 8+

workdir: ./examples/llama_finetuning

setup: |
  pip install -r requirements.txt

run: |
  python -m sky_llama.workflows create_dataset

---
name: train

resources:
  cpus: 16+
  memory: 36+
  accelerators: L4

workdir: ./examples/llama_finetuning

setup: |
  pip install -r requirements.txt

run: |
  ls sky_llama/dataset/dataset
  export CUDA_VISIBLE_DEVICES=0,1
  echo $CUDA_VISIBLE_DEVICES
  export NCCL_DEBUG=INFO
  export NCCL_IB_DISABLE=1
  torchrun --nproc-per-node 2 -m sky_llama.workflows train